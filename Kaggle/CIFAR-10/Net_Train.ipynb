{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from Read_Data import ReadData\n",
    "from Read_Data import Image_Show\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "\n",
    "Path_Label = '../../../Data/'\n",
    "File_Name = 'Train_CIFAR10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadImageData = ReadData(File_Name, Path_Label)\n",
    "print(LoadImageData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 此处根据batch_size进行数据的分组（包括测试与batch）\n",
    " 此处根据实际需要（AlexNet的最小输入）需要对Image进行resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainTestData(Data, batch_size, Train_ratio = 0.8, resize = None):\n",
    "    # 测试数据与训练数据的数量\n",
    "    Num_Train = int(len(Data) * Train_ratio)\n",
    "    Num_Test = len(Data) - Num_Train\n",
    "    print(Num_Train, Num_Test)\n",
    "    # 检测是否可被batch_size整除\n",
    "    if Num_Train%batch_size: # | Num_Test%batch_size:\n",
    "        print(\"batch_size is not suitable!\")\n",
    "        return -1\n",
    "    Batch_Train = int(Num_Train/batch_size)\n",
    "    # 此处进行图像的resize\n",
    "    Image = []\n",
    "    if resize:\n",
    "        for i in range(len(Data)):\n",
    "            # print(Data[i,1:].shape)\n",
    "            # print(Data[i,1:].reshape(3, 32, 32).transpose(2,1,0).shape)\n",
    "            # print(misc.imresize(Data[i,1:].reshape(3, 32, 32), (resize,resize)))\n",
    "            Image.append(misc.imresize(Data[i,1:].reshape(3, 32, 32), (resize,resize)).flatten())\n",
    "            # Image.append(np.hstack((Data[i,:1], misc.imresize(Data[i,1:].reshape(3, 32, 32), (resize,resize)).flatten())))\n",
    "        Image = np.array(Image)\n",
    "        Image_Data = np.concatenate((Data[:,:1], Image), axis = 1).astype(np.uint8)\n",
    "        # 之前出现颜色问题是因为整型和浮点型的原因\n",
    "        #plt.imshow(Image_Data[1,1:].reshape(resize, resize, 3))\n",
    "        #plt.show()\n",
    "        Data = Image_Data;\n",
    "    # Batch_Test = int(Num_Test/batch_size)\n",
    "    # 将训练数据放入Batch_Train个batch中\n",
    "    Train_Data = []\n",
    "    for i in range(Batch_Train):\n",
    "        Train_Data.append(Data[i*batch_size:(i+1)*batch_size, :])\n",
    "    # 将剩下的数据作为Test_Data\n",
    "    Test_Data = Data[Batch_Train* batch_size: , :]\n",
    "    return np.array(Train_Data), np.array(Test_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = np.ones((100, 3073))\n",
    "# print(Data.shape)\n",
    "Train_Data, Test_Data = loadTrainTestData(LoadImageData, 256, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(Image[2])\n",
    "#plt.show()\n",
    "# print(LoadImageData.shape)\n",
    "# print(Train_Data.shape)\n",
    "# print(Test_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetImageAndLabel(Data, size = 32):\n",
    "    Label = Data[:,0]\n",
    "    Image = Data[:,1:].reshape((len(Data),size, size, 3)).transpose((0,3,2,1))\n",
    "    #print(Image.shape)\n",
    "    return Image/255, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image, Label = GetImageAndLabel(Train_Data[0])\n",
    "# print(Image.shape)\n",
    "# print(Label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_show(Image, Label):\n",
    "    plt.imshow(Image.transpose(1,2,0))\n",
    "    plt.title(Label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Image[0].shape)\n",
    "# Image_show(Image[0], Label[0])\n",
    "#image_resize = misc.imresize(Image[0], (224,224))\n",
    "#print(image_resize.shape)\n",
    "#Image_show(image_resize.transpose(2,1,0), Label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个基础的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "from mxnet import autograd as ag\n",
    "from time import clock\n",
    "import mxnet\n",
    "ctx = mxnet.gpu()\n",
    "# # AlexNet对输入的图形的像素尺寸有要求，不能用原先的32*32\n",
    "# # AlexNet最后的全连接层需要根据实际需要，进行类别选择\n",
    "AlexNet = vision.alexnet(classes=10)\n",
    "#print(AlexNet)\n",
    "AlexNet.initialize(init=init.Xavier(),ctx=ctx)\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "Trainer = gluon.Trainer(AlexNet.collect_params(), 'sgd', {'learning_rate': 0.05})\n",
    "\n",
    "# # x = nd.random_normal(shape=(10, 3, 224, 224))\n",
    "# # print(x.shape)\n",
    "# # y = AlexNet(x)\n",
    "# # print(y.shape)\n",
    "\n",
    "# from mxnet import gluon\n",
    "# from mxnet import init\n",
    "# from mxnet.gluon import nn\n",
    "# ConvNet = nn.Sequential()\n",
    "# with ConvNet.name_scope():\n",
    "#     ConvNet.add(\n",
    "#         nn.Conv2D(channels=20, kernel_size=5, activation='relu'),\n",
    "#         nn.MaxPool2D(pool_size=2, strides=2),\n",
    "#         nn.Conv2D(channels=50, kernel_size=3, activation='relu'),\n",
    "#         nn.MaxPool2D(pool_size=2, strides=2),\n",
    "#         nn.Flatten(),\n",
    "#         nn.Dense(128, activation=\"relu\"),\n",
    "#         nn.Dense(10)\n",
    "#     )\n",
    "\n",
    "# ConvNet.initialize(init=init.Xavier(), ctx=ctx)\n",
    "\n",
    "\n",
    "\n",
    "# loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# Trainer = gluon.Trainer(AlexNet.collect_params(), 'sgd', {'learning_rate': 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, label):\n",
    "    return nd.mean(out.argmax(axis=1) == label).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(Data, net, ctx, Size):\n",
    "    test_image, test_label = GetImageAndLabel(Data, size= Size)\n",
    "    test_image = nd.array(test_image, ctx=ctx)\n",
    "    test_label = nd.array(test_label, ctx=ctx)\n",
    "    out = net(test_image)\n",
    "    # print(out.argmax(axis=1), \" \", test_label)\n",
    "    acc = accuracy(out, test_label)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, net, echoes, loss_func, trainer, size, ctx):\n",
    "    test_acc = 0\n",
    "    for echoe in range(echoes):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        start_time = clock()\n",
    "        for train_batch in train_data:\n",
    "            #print(train_batch.shape)\n",
    "            batch_size = len(train_batch)\n",
    "            train_image, train_label = GetImageAndLabel(train_batch, size=size)\n",
    "            #print(train_image.shape)\n",
    "            #print(train_label.shape)\n",
    "            train_image = nd.array(train_image, ctx=ctx)\n",
    "            train_label = nd.array(train_label, ctx=ctx)\n",
    "            with ag.record():\n",
    "                #print(train_image[0])\n",
    "                out = net(train_image)\n",
    "                #print(out.shape)\n",
    "                #print(out.argmax(axis=1))\n",
    "                #print(train_label)\n",
    "                loss = loss_func(out, train_label)\n",
    "                #print(loss)\n",
    "            # 此处需要backward()！！！！\n",
    "            loss.backward()\n",
    "            # trainer.set_learning_rate(0.1)\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += np.mean(loss.asnumpy())\n",
    "            # 局部正确率的计算\n",
    "            #print(train_label.shape)\n",
    "            #print(batch_size)\n",
    "            #print(out.argmax(axis=1))\n",
    "            #print(train_label)\n",
    "            #print(accuracy(out, train_label)/batch_size)\n",
    "            train_acc += accuracy(out, train_label)\n",
    "            # print(train_acc)\n",
    "        # 此处需要输出最后的loss及其精度\n",
    "        end_time = clock()\n",
    "        use_time = (end_time - start_time) #/1000000\n",
    "        # test_acc = evaluate_accuracy(test_data, net, ctx, size)\n",
    "        print(\"Echoe is %d, Train Loss is %f, Train Acc is %f, Test Acc is %f, Use-time is %f S\" % (echoe+1, train_loss/len(train_data), train_acc/len(train_data), test_acc, use_time))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(Train_Data, Test_Data, AlexNet, 50, loss, Trainer, 224, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0;\n",
    "# for train_batch in Train_Data:\n",
    "#     i += 1\n",
    "#     print(i)\n",
    "#     temp_2 = nd.array(train_batch, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_3 = nd.array(Train_Data, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_out = nd.array([[-0.29596519,0.78109169,0.3583461,0.3933138,0.10683483,-0.18825993,-0.17625771,-0.33768478,-0.68717098,0.20490324],\n",
    "#                     [-0.29596519,0.78109169,0.8583461,0.3933138,0.10683483,-0.18825993,-0.17625771,-0.33768478,-0.68717098,0.20490324],\n",
    "#                     [-0.29596519,0.78109169,0.3583461,0.9933138,0.10683483,-0.18825993,-0.17625771,-0.33768478,-0.68717098,0.20490324]])\n",
    "# print(temp_out.shape)\n",
    "# print(temp_out.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调试网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(X):\n",
    "#     exp = nd.exp(X)\n",
    "#     # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，\n",
    "#     # 就是返回 (nrows, 1) 形状的矩阵\n",
    "#     partition = exp.sum(axis=1, keepdims=True)\n",
    "#     return exp / partition\n",
    "# def cross_entropy(yhat, y):\n",
    "#     return - nd.pick(nd.log(yhat), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../../gluon-tutorials-zh')\n",
    "# from mxnet import ndarray as nd\n",
    "# from mxnet import autograd\n",
    "# import utils\n",
    "\n",
    "# batch_size = 256\n",
    "# train_data, test_data = utils.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mxnet import gluon\n",
    "# from mxnet import init\n",
    "# from mxnet.gluon import nn\n",
    "# ConvNet = nn.Sequential()\n",
    "# with ConvNet.name_scope():\n",
    "#     ConvNet.add(\n",
    "#         nn.Conv2D(channels=20, kernel_size=5, activation='relu'),\n",
    "#         nn.MaxPool2D(pool_size=2, strides=2),\n",
    "#         nn.Conv2D(channels=50, kernel_size=3, activation='relu'),\n",
    "#         nn.MaxPool2D(pool_size=2, strides=2),\n",
    "#         nn.Flatten(),\n",
    "#         nn.Dense(128, activation=\"relu\"),\n",
    "#         nn.Dense(10)\n",
    "#     )\n",
    "# ConvNet.initialize(init=init.Xavier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.005})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mxnet import ndarray as nd\n",
    "# from mxnet import autograd\n",
    "\n",
    "# lr = 0.5\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     test_acc = 0.\n",
    "#     train_loss = 0.\n",
    "#     train_acc = 0.\n",
    "#     #for data, label in train_data:\n",
    "#     for train_batch in Train_Data:\n",
    "#         data, label = GetImageAndLabel(train_batch, size=32)\n",
    "#         data = nd.array(data)\n",
    "#         #print(data.shape)\n",
    "#         label = nd.array(label)\n",
    "#         #print(label.shape)\n",
    "#         batch_size = len(data)\n",
    "#         with autograd.record():\n",
    "#             #print(data.shape)\n",
    "#             output = net(data)\n",
    "#             #temp_soft = softmax(output)\n",
    "#             #temp_loss = cross_entropy(temp_soft, label)\n",
    "#             #print(output)\n",
    "#             #print(output.argmax(axis=1))\n",
    "#             #print(label)\n",
    "#             #print(temp_soft)\n",
    "#             #print(temp_loss)\n",
    "            \n",
    "#             #print(label.shape)\n",
    "            \n",
    "#             # loss计算有问题\n",
    "#             loss = softmax_cross_entropy(output, label)\n",
    "#             #print(loss)\n",
    "#             #print(loss.shape)\n",
    "#         loss.backward()\n",
    "#         trainer.step(batch_size)\n",
    "        \n",
    "#         trainer.set_learning_rate(lr)\n",
    "#         train_loss += nd.mean(loss).asscalar()\n",
    "#         train_acc += accuracy(output, label)\n",
    "#     lr = lr * 0.9\n",
    "#     test_acc = evaluate_accuracy(nd.array(Test_Data), net)\n",
    "#     print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f, lr is %f\" % (\n",
    "#         epoch, train_loss/len(Train_Data), train_acc/len(Train_Data), test_acc, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
